Week9a
       (Problem, Part): attempt_count
        (u'1', u'1'): 252,
        (u'1', u'2'): 380,
        (u'2', u'1'): 902,
        (u'2', u'2'): 232,
        (u'2', u'3'): 80,
        (u'2', u'4'): 423,
        (u'2', u'5'): 681,
        (u'2', u'6'): 715,
        (u'2', u'7'): 895,
        (u'3', u'1'): 455,
        (u'4', u'1'): 246,
        (u'4', u'2'): 22,
        (u'4', u'3'): 58,
        (u'4', u'4'): 28,
        (u'4', u'5'): 44,
        (u'4', u'6'): 65,
        (u'5', u'1'): 508,
        (u'6', u'1'): 7,
        (u'6', u'2'): 14,
        (u'6', u'3'): 187,
        (u'7', u'1'): 107,
        (u'7', u'2'): 526,
        (u'7', u'3'): 538,
        (u'8', u'1'): 363,
        (u'8', u'2'): 108,
        (u'8', u'3'): 137,
        (u'8', u'4'): 278,
        (u'9', u'1'): 595,
        (u'9', u'2'): 91,
        (u'9', u'3'): 591
        (u'10', u'1'): 1112,
        (u'10', u'2'): 687,
        (u'10', u'3'): 291,
        (u'11', u'1'): 261,
        (u'11', u'2'): 152,
        (u'11', u'3'): 1006,
        (u'11', u'4'): 199,
        (u'11', u'5'): 175,
        (u'11', u'6'): 724,
        (u'11', u'7'): 235,
        (u'12', u'1'): 185,
        (u'13', u'1'): 834,
        (u'13', u'2'): 461,
        (u'13', u'3'): 1577,
        (u'13', u'4'): 144,
        (u'13', u'5'): 297,
        (u'13', u'6'): 202,
        (u'14', u'1'): 740,
        (u'14', u'2'): 806,
        (u'14', u'3'): 219,
        (u'14', u'4'): 270,
        (u'15', u'1'): 172,
        (u'15', u'2'): 68,
        (u'15', u'3'): 179,
        (u'15', u'4'): 207,
        (u'15', u'5'): 90,
        (u'15', u'6'): 126,


Assigned problem file name:
+--------+------------+-----------------------------------------------+
| set_id | problem_id | source_file                                   |
+--------+------------+-----------------------------------------------+
| Week9a |          1 | local/Reorganized/RandomizedAlgs/min_hash1.pg |
| Week9a |          2 | local/Reorganized/RandomizedAlgs/min_hash2.pg |
| Week9a |          3 | Reorganized/RandomizedAlgs/Hashing_6.pg       |
+--------+------------+-----------------------------------------------+

Problem 1



Problem 2




Problem 3
    $a = Compute("0.05/sqrt(0.16/200)");
    $q = Compute("Q($a)");

    ## Detecting near-duplicates ##

    Near-duplication is pervasive in the web: there are large numbers of distinct URLs which have exactly
    the same content but differ only in unimportant details like headers and footers. The user of a search
    engine would not be pleased if the answer to his query was a set of 10 near-identical pages! In order
    to remove this redundancy, we need to define a notion of _similarity_ between documents.

    #### The similarity between two documents ####

    For any document---call it [`d`]---let the set of all words in [`d`] be denoted [`C(d)`]. For two documents
    [`d`] and [`d'`], we will measure their similarity by the function

    [$BCENTER]*
    [`` S(d,d') \ = \ \frac{|C(d) \cap C(d')|}{|C(d) \cup C(d')|} .``]
    [$ECENTER]*

    If the two documents are truly identical, [`S(d,d') = 1`]. If they are almost-identical, [`S(d,d')`] will
    be close to 1. And if they are completely different, with no words in common, then [`S(d,d')`] will be
    zero. We'll consider [`d`] and [`d'`] to be near-duplicates if [`S(d,d')`] is sufficiently close to 1.

    Now, imagine a search engine that is going through a list of documents or webpages, and wants to
    eliminate near-duplicates. Here's an algorithm it could use:

    * [`{\cal D} = \emptyset`] (set of documents, initially empty)
    * for each document [`d`] that appears:
        - if [`S(d,d')`] is significantly smaller than 1 for all [`d'`] in [`{\cal D}`]: add [`d`] to [`{\cal D}`]

    The final set of documents [`{\cal D}`] will contain no near-duplicates. This is good, but the
    algorithm is very slow. Suppose for the sake of simplicity that there are [`n`] documents in total,
    each of length [`L`]. Then computing the similarity between two documents takes [`O(L)`] time, and there are [________]{(n-1)n/2} pairs, thus the algorithm takes [`O(n^2L)`]. This quadratic dependence on [`n`] is prohibitive in web-scale applications,
    where [`n`] could easily be in the billions or tens of billions.

    To get a faster algorithm, we once again resort to hashing.

    ---

    Again for the example:

    A: [`\verb|Thanksgiving may be the perfect day to give thanks, but what about giving thanks to a delicious and flawless turkey?|`]

    B: [`\verb|Pick the perfect centerpiece for your Thanksgiving or holiday meal from our ultimate collection of roast turkey recipes.|`]



    After preprocessing, the document A is represented by the *set*  [`\{\verb|Thanksgiving, perfect, day, give, thanks, delicious, flawless, turkey|\}`], and

    document B becomes the *set* [`\{\verb|pick, perfect, centerpiece, Thanksgiving, holiday, meal, ultimate, collection, roast, turkey, recipe |\}`]


    The similarity between A and B is [__________]{"3/(8+11-3)"}.
